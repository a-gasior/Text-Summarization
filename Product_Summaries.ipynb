{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product_Summaries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-gasior/Text-Summarization/blob/master/Product_Summaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0C266BgR6dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alari Maricq\n",
        "#Andrew Gasiorowski\n",
        "#CIS 411"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgY5CM_TxE4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bjGxxM0xKTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as _np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import pairwise_distances_argmin#_min\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from operator import itemgetter\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74UscfCMvdkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NGRAMS = 2\n",
        "def make_ngrams(text, ngrams=NGRAMS):\n",
        "    \"\"\"Breaks array of words into array of n-tuples representing N-Grams\"\"\"\n",
        "    if ngrams < 1:\n",
        "        raise ValueError(\"ngrams must be greater than 0\")\n",
        "\n",
        "    text = word_tokenize(text)\n",
        "    text = [x for x in text if x not in string.punctuation]\n",
        "    grams = []\n",
        "    off = ngrams - 1  # offset for 0-indexing\n",
        "    for ind in range(off, len(text)):\n",
        "        grams.append(tuple(text[ind - off:ind + 1]))\n",
        "    return grams\n",
        "\n",
        "\n",
        "def rouge_comp(tests, gold, ngrams=NGRAMS):\n",
        "    \"\"\"Compare generated test summaries to gold standard using ROUGE-N\n",
        "\n",
        "    Args:\n",
        "        test: list of system generated summary strings\n",
        "        gold: list of gold-standard summary strings.\n",
        "        ngrams (opt): size of N-Grams to tokenize summaries into.\n",
        "\n",
        "    Returns: list of floats for the ROUGE recall score of each test summary\n",
        "    \"\"\"\n",
        "    if not isinstance(tests, list):\n",
        "        tests = [tests]\n",
        "\n",
        "    goldngrams = [make_ngrams(x, ngrams) for x in gold]\n",
        "    scores = []\n",
        "    for test in tests:\n",
        "        grams = make_ngrams(test)\n",
        "        # sum of common words between test and each reference (gold)\n",
        "        # print(grams)\n",
        "        # print(goldngrams)\n",
        "        matches = sum(len(set(grams) & set(x)) for x in goldngrams)\n",
        "        # print(matches)\n",
        "        total_gold = sum(len(x) for x in goldngrams)\n",
        "        scores.append(matches / total_gold)\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hSY02VlxSe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#takes the name of a directory for a topic\n",
        "#returns list of sentences (product reviews) AND list of gold standard summaries for this topic\n",
        "def get_data(topic_dir):\n",
        "  filename_topic = 'gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/topics/' + topic_dir\n",
        "  filename_goldstnd = 'gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/summaries-gold/' + topic_dir + '/'\n",
        "  sentences_topic = []\n",
        "  sentences_goldstnd = []\n",
        "  \n",
        "  #get list of product reviews for this topic\n",
        "  with open(filename_topic, errors='replace') as f:\n",
        "    for line in f:\n",
        "      cleaned = line.replace('\\n', '')\n",
        "      cleaned = re.sub(r'[^\\w\\s]','',cleaned)\n",
        "      sentences_topic.append(cleaned)\n",
        "      #append to the list of reviews\n",
        "      \n",
        "  gold_topic = topic_dir.replace('.txt.data','')\n",
        "  gold_list = os.listdir('gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/summaries-gold/' + gold_topic)\n",
        "  #There may be many gold standard summaries. Get list of all summary file names\n",
        "  sentences_goldstnd = []\n",
        "  for gld_std in gold_list:\n",
        "    #for every gold standard summary file\n",
        "    filename_goldstnd = 'gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/summaries-gold/' + gold_topic + '/' + gld_std\n",
        "    with open(filename_goldstnd, errors='ignore') as f:\n",
        "      for line in f:\n",
        "        #for every line in this gold standard summary\n",
        "        cleaned = line.replace('\\n', '')\n",
        "        cleaned = re.sub(r'[^\\w\\s]','',cleaned)\n",
        "        sentences_goldstnd.append(cleaned)\n",
        "        #append the line to the summary list\n",
        "  \n",
        "  #return the product reviews AND the gold standard summaries\n",
        "  return sentences_topic, sentences_goldstnd\n",
        "\n",
        "\n",
        "#Takes the name of a .npy file\n",
        "#Returns ndarray of word embeddings\n",
        "def get_embedding_from_disk(_file_name):\n",
        "  _file_path = 'gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/topics_embeddings/'+_file_name\n",
        "  return _np.load(_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gHOUhNzaaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Takes a topic's word embeddings\n",
        "#Returns list of best sentences, and optionally thier indexes in the topic\n",
        "#determines this using kmeans clustering\n",
        "def summarize_topic(topic_embeddings, sentences, num_clusters):\n",
        "  #Establish parameters for clustering\n",
        "  kmeans = KMeans(n_clusters=num_clusters)\n",
        "  kmeans.fit(topic_embeddings)\n",
        "  #closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, topic_embeddings)\n",
        "  closest = pairwise_distances_argmin(kmeans.cluster_centers_, topic_embeddings)\n",
        "  return [sentences[i] for i in closest], closest\n",
        "\n",
        "\n",
        "#Takes list of tuples(alpha, acc) and range of clusters\n",
        "#returns list of tuples(alpha, avg_acc)\n",
        "#returns max score value\n",
        "def aggregate_accuracy(acc_list, cluster_range):\n",
        "  avg_acc_list = Counter()\n",
        "  \n",
        "  for i in cluster_range:\n",
        "    for tup in acc_list:\n",
        "      if tup[0] == i:\n",
        "        avg_acc_list[i] += tup[1]\n",
        "  num_samples = len(acc_list_all_topics)/max(cluster_range)\n",
        "  for key, value in avg_acc_list.items():\n",
        "    avg_acc_list[key] = value/num_samples\n",
        "  avg_acc_list = [(k, v) for k, v in avg_acc_list.items()]\n",
        "  max_alpha = max(avg_acc_list,key=itemgetter(1))[0]\n",
        "  \n",
        "  return avg_acc_list, max_alpha\n",
        "\n",
        "#takes avg_acc_list\n",
        "#saves chart to file\n",
        "def save_plot_to_file(data, file_name, title):\n",
        "  filename = 'gdrive/Team Drives/NLP_Text_Summarizer/Data/data_out/new_' + file_name + '.png'\n",
        "  title = title\n",
        "  plt.scatter(*zip(*data))\n",
        "  plt.title(title)\n",
        "  plt.xlabel(\"Number of Clusters\")\n",
        "  plt.ylabel(\"ROUGE Score\")\n",
        "  plt.savefig(filename ,bbox_inches='tight')\n",
        "  plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKOpVfh-xijK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#each list item is the file name of a .npy containing a topic's word embeddings\n",
        "embedding_file_list = os.listdir('gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/topics_embeddings')\n",
        "topics_list = os.listdir('gdrive/Team Drives/NLP_Text_Summarizer/Data/opinosis/topics')\n",
        "embedding_file_list.sort()\n",
        "topics_list.sort()\n",
        "\n",
        "\n",
        "#split training and testing data:\n",
        "training_topics = topics_list[:36]\n",
        "testing_topics = topics_list[36:]\n",
        "#Specifiy N-Gram type for ROUGE-N compare\n",
        "\n",
        "#prepare test parameters\n",
        "cluster_range = range(1,11)\n",
        "validation_range = range(1,101)\n",
        "num_validations = max(validation_range)\n",
        "\n",
        "#Basically what we're doing is the following:\n",
        "#We want to find out what the optimal number of clusters is\n",
        "#Because we want a summary we decided it shouldn't be any longer than 10 sentences (ten clusters)\n",
        "#thus we learn the optimal number of clusters\n",
        "#We do this by, for every number of clusters\n",
        "#Run the clustering algorithm 100 times\n",
        "#Keeping track of the ROUGE score for each run\n",
        "#we then average the ROUGE score accross the 100 runs for that number of clusters\n",
        "#This allows us to figure out the optimal number of clusters-->turns out it's two\n",
        "\n",
        "acc_list_all_topics = []    \n",
        "for idx, topic in enumerate(training_topics):\n",
        "    #for every training topic\n",
        "    print('summarizing', topic)\n",
        "    acc_dict = dict.fromkeys(cluster_range,0)\n",
        "    for num_clusters in cluster_range:\n",
        "        for this_validation in validation_range:\n",
        "            #run this N times to find average ROUGE score\n",
        "            this_topic_embedings = get_embedding_from_disk(embedding_file_list[idx])\n",
        "            #gets sentence embeddings from file\n",
        "            sentences, gold_summaries = get_data(topic)\n",
        "            #gets corresponding raw text AND gold standard summaries for this topic\n",
        "            this_topic_best_sentences = summarize_topic(this_topic_embedings, sentences, num_clusters)\n",
        "            #summarize the topic\n",
        "            scores_train = rouge_comp(this_topic_best_sentences[0], gold_summaries)\n",
        "            #use ROUGE-N to calculate score\n",
        "            acc_dict[num_clusters] += (sum(scores_train)/len(scores_train))*100\n",
        "            #update the running total for the score\n",
        "    acc_dict.update((key, value/num_validations) for key, value in acc_dict.items())\n",
        "    #divide the sums by the number of validations to get the average score by  number of clusters\n",
        "    acc_list_all_topics.extend([(key, value) for key, value in acc_dict.items()])\n",
        "avg_accs_train, optimal_clust = aggregate_accuracy(acc_list_all_topics, cluster_range)\n",
        "save_plot_to_file(avg_accs_train, 'training_summary', 'Training Topics')\n",
        "\n",
        "\n",
        "\n",
        "#now that we've learned the optimal number of clusters(2) we can test our test data\n",
        "#We will do something similar to what we did above\n",
        "#For every topic in the test set\n",
        "#We will run the clustering algorithm 100 times\n",
        "#The clustering algorithm won't always return the same sentences\n",
        "#By running it 100 times and selecting the top 2 that appear most frequently\n",
        "#We have the best chance of selecting the ideal summary\n",
        "#We then print the metrics to output\n",
        "\n",
        "acc_list_test = []\n",
        "avg_score = 0\n",
        "for idx, topic in enumerate(testing_topics):\n",
        "  this_topic_embedings = get_embedding_from_disk(embedding_file_list[idx + 36])\n",
        "  sentences, gold_summaries = get_data(topic)\n",
        "  best_indexes_dict = Counter()\n",
        "  for validation_iter in validation_range:\n",
        "    this_topic_best_sentences, best_idx = summarize_topic(this_topic_embedings, sentences, optimal_clust)\n",
        "    for sent_idx in best_idx:\n",
        "      best_indexes_dict[sent_idx] = sent_idx\n",
        "  topic_best_sentence_idx = best_indexes_dict.most_common(optimal_clust)\n",
        "  #list of tuple. tuple[0] is a best index  \n",
        "  best_sentences_test = []\n",
        "  for sent_idx in topic_best_sentence_idx:\n",
        "    best_sentences_test.append(sentences[sent_idx[0]])\n",
        "  score = rouge_comp(best_sentences_test, gold_summaries)\n",
        "  score = (sum(score)/len(score))*100\n",
        "  avg_score += score\n",
        "  print('Topic: ', topic, '\\nAccuracy: ', score,'\\n\\nSystem Generated Summary:\\n')\n",
        "  for sent in best_sentences_test:\n",
        "    print(sent, end='\\n\\n')\n",
        "  print('\\nGold Standard Summary:\\n')\n",
        "  for sent in gold_summaries:\n",
        "    print(sent)\n",
        "  print('\\n\\n\\n-----------------NEW TOPIC---------------\\n\\n\\n')\n",
        "avg_score = avg_score/14\n",
        "print('Average Rouge Score: ', avg_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtBPJFBe9EoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Lr8Lu5DFGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMvXVxXkDKBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ZgCHPJDKnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b7C_GdnObcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}